{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#sklearn imports:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in fresh cleaned dataset\n",
    "df = pd.read_csv('chicago_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4273756 entries, 0 to 4273755\n",
      "Data columns (total 15 columns):\n",
      "Date                    object\n",
      "ID                      int64\n",
      "Block                   int64\n",
      "Primary Type            int64\n",
      "Description             int64\n",
      "Location Description    int64\n",
      "Arrest                  bool\n",
      "Domestic                bool\n",
      "District                float64\n",
      "Year                    int64\n",
      "Latitude                float64\n",
      "Longitude               float64\n",
      "Month                   int64\n",
      "Day                     int64\n",
      "Hour                    int64\n",
      "dtypes: bool(2), float64(3), int64(9), object(1)\n",
      "memory usage: 432.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ID</th>\n",
       "      <th>Block</th>\n",
       "      <th>Primary Type</th>\n",
       "      <th>Description</th>\n",
       "      <th>Location Description</th>\n",
       "      <th>Arrest</th>\n",
       "      <th>Domestic</th>\n",
       "      <th>District</th>\n",
       "      <th>Year</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-04-02 13:00:00</td>\n",
       "      <td>4673626</td>\n",
       "      <td>23279</td>\n",
       "      <td>12</td>\n",
       "      <td>173</td>\n",
       "      <td>66</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>41.981913</td>\n",
       "      <td>-87.771996</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-02-26 13:40:48</td>\n",
       "      <td>4673627</td>\n",
       "      <td>26672</td>\n",
       "      <td>10</td>\n",
       "      <td>217</td>\n",
       "      <td>75</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>41.775733</td>\n",
       "      <td>-87.611920</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-08 23:16:00</td>\n",
       "      <td>4673628</td>\n",
       "      <td>6596</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>58</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>41.769897</td>\n",
       "      <td>-87.593671</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date       ID  Block  Primary Type  Description  \\\n",
       "0  2006-04-02 13:00:00  4673626  23279            12          173   \n",
       "1  2006-02-26 13:40:48  4673627  26672            10          217   \n",
       "2  2006-01-08 23:16:00  4673628   6596             0           40   \n",
       "\n",
       "   Location Description  Arrest  Domestic  District  Year   Latitude  \\\n",
       "0                    66   False     False      16.0  2006  41.981913   \n",
       "1                    75    True     False       3.0  2006  41.775733   \n",
       "2                    58   False     False       3.0  2006  41.769897   \n",
       "\n",
       "   Longitude  Month  Day  Hour  \n",
       "0 -87.771996      4    2    13  \n",
       "1 -87.611920      2   26    13  \n",
       "2 -87.593671      1    8    23  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_model = df.drop(['ID','Location Description', 'Latitude', 'Longitude', 'Date'], axis=1)\n",
    "\n",
    "labels = df['Location Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4273756 entries, 0 to 4273755\n",
      "Data columns (total 10 columns):\n",
      "Block           int64\n",
      "Primary Type    int64\n",
      "Description     int64\n",
      "Arrest          bool\n",
      "Domestic        bool\n",
      "District        float64\n",
      "Year            int64\n",
      "Month           int64\n",
      "Day             int64\n",
      "Hour            int64\n",
      "dtypes: bool(2), float64(1), int64(7)\n",
      "memory usage: 269.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_for_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_features(X, y, key=0):\n",
    "    \"\"\"\n",
    "    General helper function for evaluating effectiveness of passed features in ML model\n",
    "\n",
    "    Prints out Log loss, accuracy, and confusion matrix with 3-fold stratified cross-validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Features array\n",
    "\n",
    "    y : Labels array\n",
    "\n",
    "    key: 0 = DecisionTreeClassifier (Default)\n",
    "         1 = ExtraTreeClassifier\n",
    "         2 = RandomForestClassifier\n",
    "         3 = KNeighborsClassifier\n",
    "         4 = GaussianNB\n",
    "    \"\"\"\n",
    "\n",
    "    clf = [DecisionTreeClassifier(),\n",
    "           ExtraTreeClassifier(),\n",
    "           RandomForestClassifier(),\n",
    "           KNeighborsClassifier(),\n",
    "           GaussianNB()]\n",
    "    \n",
    "\n",
    "    probabilities = cross_val_predict(clf[key], X, y, cv=StratifiedKFold(n_splits=2, random_state=8),\n",
    "                                      n_jobs=-1, method='predict_proba', verbose=2)\n",
    "    predicted_indices = np.argmax(probabilities, axis=1)\n",
    "    classes = np.unique(y)\n",
    "    predicted = classes[predicted_indices]\n",
    "    print('Log loss: {}'.format(log_loss(y, probabilities)))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y, predicted)))\n",
    "    #skplt.metrics.plot_confusion_matrix(y, predicted, normalize=True, figsize=(20,10))\n",
    "    \n",
    "    print(classification_report(y, predicted))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 33.25500208494404\n",
      "Accuracy: 0.03714999171688791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.04      0.01      8626\n",
      "           1       0.00      0.05      0.00       541\n",
      "           2       0.00      0.05      0.00       515\n",
      "           3       0.00      0.16      0.00       498\n",
      "           4       0.04      0.27      0.06      1225\n",
      "           5       0.01      0.05      0.01       472\n",
      "           6       0.07      0.35      0.12      3336\n",
      "           7       0.00      0.40      0.01       631\n",
      "           8       0.04      0.67      0.08      8948\n",
      "           9       0.03      0.00      0.01     96996\n",
      "          10       0.00      0.01      0.00       569\n",
      "          11       0.05      0.01      0.01    481218\n",
      "          12       0.00      0.00      0.00      1075\n",
      "          13       0.04      0.09      0.05      5907\n",
      "          14       0.01      0.02      0.01      6239\n",
      "          15       0.09      0.18      0.12       623\n",
      "          16       0.11      0.09      0.10     17069\n",
      "          17       0.08      0.04      0.06     24416\n",
      "          18       0.00      0.01      0.00      5015\n",
      "          19       0.02      0.00      0.00      1641\n",
      "          20       0.01      0.04      0.01     15825\n",
      "          21       0.01      0.35      0.03     12148\n",
      "          22       0.01      0.25      0.02     24287\n",
      "          23       0.02      0.00      0.00      9684\n",
      "          24       0.00      0.02      0.00      2529\n",
      "          25       0.00      0.03      0.00       595\n",
      "          26       0.03      0.01      0.02      3692\n",
      "          27       0.06      0.02      0.03      1010\n",
      "          28       0.03      0.06      0.04     27023\n",
      "          29       0.01      0.25      0.02      8370\n",
      "          30       0.02      0.20      0.03     11868\n",
      "          31       0.00      0.00      0.00     14937\n",
      "          32       0.00      0.07      0.00      5055\n",
      "          33       0.02      0.01      0.02      8338\n",
      "          34       0.40      0.14      0.20     23655\n",
      "          35       0.00      0.37      0.00      1929\n",
      "          36       0.02      0.03      0.03     16373\n",
      "          37       0.24      0.10      0.14      6756\n",
      "          38       0.01      0.00      0.00      1761\n",
      "          39       0.00      0.10      0.00       492\n",
      "          40       0.46      0.14      0.22     52752\n",
      "          41       0.00      0.00      0.00     12631\n",
      "          42       0.11      0.16      0.13     17535\n",
      "          43       0.00      0.04      0.01      3896\n",
      "          44       0.00      0.00      0.00       458\n",
      "          45       0.01      0.00      0.00       622\n",
      "          46       0.18      0.03      0.06     45329\n",
      "          47       0.04      0.01      0.02      8422\n",
      "          48       0.32      0.12      0.17     49901\n",
      "          49       0.00      0.00      0.00       629\n",
      "          50       0.07      0.02      0.03     12618\n",
      "          51       0.33      0.07      0.11     16701\n",
      "          52       0.01      0.02      0.02       813\n",
      "          53       0.00      0.01      0.00       779\n",
      "          54       0.22      0.05      0.08      4083\n",
      "          55       0.01      0.00      0.00      4427\n",
      "          56       0.02      0.01      0.02      1459\n",
      "          57       0.01      0.00      0.00      7894\n",
      "          58       0.05      0.01      0.02    154095\n",
      "          59       0.09      0.03      0.05      1716\n",
      "          60       0.01      0.08      0.02      5618\n",
      "          61       0.20      0.04      0.07      3496\n",
      "          62       0.02      0.01      0.01     33963\n",
      "          63       0.01      0.00      0.00    119365\n",
      "          64       0.20      0.06      0.09     11204\n",
      "          65       0.00      0.03      0.00       652\n",
      "          66       0.36      0.05      0.09    696481\n",
      "          67       0.03      0.00      0.00     73989\n",
      "          68       0.13      0.03      0.05     84099\n",
      "          69       0.01      0.15      0.02     56020\n",
      "          70       0.11      0.03      0.04     64907\n",
      "          71       0.02      0.01      0.01      8244\n",
      "          72       0.00      0.00      0.00      2645\n",
      "          73       0.09      0.04      0.06     89922\n",
      "          74       0.04      0.01      0.01     19311\n",
      "          75       0.09      0.01      0.01    478209\n",
      "          76       0.11      0.03      0.05     75120\n",
      "          77       0.01      0.01      0.01      3574\n",
      "          78       0.27      0.04      0.06   1069619\n",
      "          79       0.02      0.06      0.02     12215\n",
      "          80       0.04      0.07      0.05      4825\n",
      "          81       0.00      0.00      0.00     16431\n",
      "          82       0.09      0.02      0.03     76405\n",
      "          83       0.01      0.00      0.00      3460\n",
      "          84       0.02      0.02      0.02      5335\n",
      "\n",
      "   micro avg       0.04      0.04      0.04   4273756\n",
      "   macro avg       0.06      0.07      0.03   4273756\n",
      "weighted avg       0.17      0.04      0.05   4273756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(df_for_model, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   37.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 31.45233191228527\n",
      "Accuracy: 0.08931464501015032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.04      0.01      8626\n",
      "           1       0.00      0.04      0.00       541\n",
      "           2       0.00      0.03      0.01       515\n",
      "           3       0.00      0.08      0.00       498\n",
      "           4       0.03      0.15      0.04      1225\n",
      "           5       0.01      0.04      0.01       472\n",
      "           6       0.08      0.44      0.13      3336\n",
      "           7       0.01      0.41      0.02       631\n",
      "           8       0.03      0.60      0.06      8948\n",
      "           9       0.05      0.01      0.02     96996\n",
      "          10       0.00      0.00      0.00       569\n",
      "          11       0.23      0.12      0.16    481218\n",
      "          12       0.01      0.01      0.01      1075\n",
      "          13       0.03      0.08      0.05      5907\n",
      "          14       0.05      0.11      0.07      6239\n",
      "          15       0.03      0.14      0.05       623\n",
      "          16       0.10      0.05      0.07     17069\n",
      "          17       0.10      0.07      0.08     24416\n",
      "          18       0.00      0.00      0.00      5015\n",
      "          19       0.00      0.00      0.00      1641\n",
      "          20       0.01      0.04      0.01     15825\n",
      "          21       0.01      0.33      0.03     12148\n",
      "          22       0.01      0.20      0.02     24287\n",
      "          23       0.01      0.00      0.00      9684\n",
      "          24       0.00      0.02      0.00      2529\n",
      "          25       0.00      0.01      0.00       595\n",
      "          26       0.02      0.01      0.01      3692\n",
      "          27       0.01      0.01      0.01      1010\n",
      "          28       0.02      0.02      0.02     27023\n",
      "          29       0.01      0.18      0.01      8370\n",
      "          30       0.01      0.13      0.02     11868\n",
      "          31       0.01      0.01      0.01     14937\n",
      "          32       0.00      0.12      0.01      5055\n",
      "          33       0.03      0.03      0.03      8338\n",
      "          34       0.32      0.13      0.18     23655\n",
      "          35       0.00      0.24      0.00      1929\n",
      "          36       0.03      0.05      0.04     16373\n",
      "          37       0.15      0.06      0.08      6756\n",
      "          38       0.00      0.00      0.00      1761\n",
      "          39       0.00      0.06      0.00       492\n",
      "          40       0.36      0.15      0.21     52752\n",
      "          41       0.00      0.01      0.00     12631\n",
      "          42       0.12      0.16      0.14     17535\n",
      "          43       0.00      0.02      0.00      3896\n",
      "          44       0.00      0.00      0.00       458\n",
      "          45       0.00      0.00      0.00       622\n",
      "          46       0.09      0.03      0.04     45329\n",
      "          47       0.05      0.02      0.02      8422\n",
      "          48       0.30      0.17      0.22     49901\n",
      "          49       0.00      0.00      0.00       629\n",
      "          50       0.03      0.01      0.02     12618\n",
      "          51       0.20      0.06      0.09     16701\n",
      "          52       0.01      0.01      0.01       813\n",
      "          53       0.00      0.01      0.01       779\n",
      "          54       0.05      0.01      0.02      4083\n",
      "          55       0.00      0.00      0.00      4427\n",
      "          56       0.01      0.00      0.01      1459\n",
      "          57       0.04      0.01      0.02      7894\n",
      "          58       0.05      0.01      0.02    154095\n",
      "          59       0.01      0.01      0.01      1716\n",
      "          60       0.01      0.07      0.02      5618\n",
      "          61       0.06      0.02      0.03      3496\n",
      "          62       0.03      0.01      0.02     33963\n",
      "          63       0.05      0.02      0.02    119365\n",
      "          64       0.19      0.10      0.13     11204\n",
      "          65       0.00      0.01      0.00       652\n",
      "          66       0.38      0.11      0.17    696481\n",
      "          67       0.04      0.01      0.01     73989\n",
      "          68       0.14      0.04      0.07     84099\n",
      "          69       0.02      0.21      0.03     56020\n",
      "          70       0.08      0.03      0.05     64907\n",
      "          71       0.01      0.01      0.01      8244\n",
      "          72       0.00      0.00      0.00      2645\n",
      "          73       0.21      0.12      0.15     89922\n",
      "          74       0.03      0.01      0.02     19311\n",
      "          75       0.24      0.06      0.09    478209\n",
      "          76       0.13      0.05      0.07     75120\n",
      "          77       0.00      0.01      0.01      3574\n",
      "          78       0.40      0.12      0.18   1069619\n",
      "          79       0.02      0.06      0.03     12215\n",
      "          80       0.02      0.03      0.02      4825\n",
      "          81       0.01      0.02      0.01     16431\n",
      "          82       0.05      0.02      0.03     76405\n",
      "          83       0.00      0.00      0.00      3460\n",
      "          84       0.01      0.01      0.01      5335\n",
      "\n",
      "   micro avg       0.09      0.09      0.09   4273756\n",
      "   macro avg       0.06      0.07      0.04   4273756\n",
      "weighted avg       0.25      0.09      0.12   4273756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(df_for_model, labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 26.69653852821965\n",
      "Accuracy: 0.03928651986683376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.03      0.01      8626\n",
      "           1       0.00      0.06      0.00       541\n",
      "           2       0.00      0.01      0.00       515\n",
      "           3       0.00      0.15      0.01       498\n",
      "           4       0.13      0.40      0.19      1225\n",
      "           5       0.01      0.03      0.02       472\n",
      "           6       0.11      0.48      0.18      3336\n",
      "           7       0.02      0.52      0.04       631\n",
      "           8       0.03      0.74      0.06      8948\n",
      "           9       0.02      0.00      0.00     96996\n",
      "          10       0.00      0.00      0.00       569\n",
      "          11       0.06      0.01      0.02    481218\n",
      "          12       0.01      0.00      0.00      1075\n",
      "          13       0.03      0.05      0.04      5907\n",
      "          14       0.03      0.05      0.04      6239\n",
      "          15       0.03      0.07      0.04       623\n",
      "          16       0.12      0.09      0.10     17069\n",
      "          17       0.04      0.01      0.02     24416\n",
      "          18       0.00      0.00      0.00      5015\n",
      "          19       0.00      0.00      0.00      1641\n",
      "          20       0.01      0.05      0.01     15825\n",
      "          21       0.02      0.41      0.03     12148\n",
      "          22       0.01      0.25      0.02     24287\n",
      "          23       0.01      0.00      0.00      9684\n",
      "          24       0.00      0.01      0.00      2529\n",
      "          25       0.00      0.02      0.01       595\n",
      "          26       0.01      0.00      0.00      3692\n",
      "          27       0.00      0.00      0.00      1010\n",
      "          28       0.03      0.05      0.03     27023\n",
      "          29       0.01      0.31      0.02      8370\n",
      "          30       0.01      0.19      0.03     11868\n",
      "          31       0.00      0.00      0.00     14937\n",
      "          32       0.00      0.19      0.01      5055\n",
      "          33       0.05      0.02      0.03      8338\n",
      "          34       0.33      0.05      0.08     23655\n",
      "          35       0.00      0.41      0.01      1929\n",
      "          36       0.02      0.02      0.02     16373\n",
      "          37       0.30      0.04      0.07      6756\n",
      "          38       0.01      0.00      0.00      1761\n",
      "          39       0.00      0.07      0.00       492\n",
      "          40       0.27      0.03      0.05     52752\n",
      "          41       0.00      0.00      0.00     12631\n",
      "          42       0.08      0.09      0.09     17535\n",
      "          43       0.00      0.03      0.01      3896\n",
      "          44       0.00      0.00      0.00       458\n",
      "          45       0.00      0.00      0.00       622\n",
      "          46       0.09      0.01      0.01     45329\n",
      "          47       0.05      0.00      0.01      8422\n",
      "          48       0.32      0.07      0.11     49901\n",
      "          49       0.00      0.00      0.00       629\n",
      "          50       0.03      0.00      0.00     12618\n",
      "          51       0.33      0.02      0.03     16701\n",
      "          52       0.00      0.00      0.00       813\n",
      "          53       0.00      0.00      0.00       779\n",
      "          54       0.24      0.01      0.02      4083\n",
      "          55       0.00      0.00      0.00      4427\n",
      "          56       0.02      0.00      0.00      1459\n",
      "          57       0.02      0.00      0.00      7894\n",
      "          58       0.03      0.00      0.00    154095\n",
      "          59       0.05      0.00      0.01      1716\n",
      "          60       0.03      0.06      0.04      5618\n",
      "          61       0.23      0.01      0.02      3496\n",
      "          62       0.01      0.00      0.00     33963\n",
      "          63       0.01      0.00      0.00    119365\n",
      "          64       0.21      0.02      0.04     11204\n",
      "          65       0.00      0.00      0.00       652\n",
      "          66       0.36      0.05      0.09    696481\n",
      "          67       0.01      0.00      0.00     73989\n",
      "          68       0.15      0.03      0.05     84099\n",
      "          69       0.01      0.23      0.03     56020\n",
      "          70       0.02      0.00      0.00     64907\n",
      "          71       0.01      0.00      0.00      8244\n",
      "          72       0.00      0.00      0.00      2645\n",
      "          73       0.20      0.07      0.10     89922\n",
      "          74       0.01      0.00      0.00     19311\n",
      "          75       0.05      0.00      0.01    478209\n",
      "          76       0.03      0.00      0.00     75120\n",
      "          77       0.02      0.01      0.01      3574\n",
      "          78       0.33      0.05      0.09   1069619\n",
      "          79       0.01      0.02      0.01     12215\n",
      "          80       0.03      0.02      0.02      4825\n",
      "          81       0.00      0.00      0.00     16431\n",
      "          82       0.13      0.01      0.01     76405\n",
      "          83       0.00      0.00      0.00      3460\n",
      "          84       0.00      0.00      0.00      5335\n",
      "\n",
      "   micro avg       0.04      0.04      0.04   4273756\n",
      "   macro avg       0.06      0.07      0.02   4273756\n",
      "weighted avg       0.18      0.04      0.05   4273756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(df_for_model,labels,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 13.617796118775468\n",
      "Accuracy: 0.34149937432085503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.02      0.01      8626\n",
      "           1       0.05      0.11      0.07       541\n",
      "           2       0.06      0.09      0.07       515\n",
      "           3       0.10      0.15      0.12       498\n",
      "           4       0.37      0.52      0.43      1225\n",
      "           5       0.04      0.03      0.03       472\n",
      "           6       0.60      0.75      0.67      3336\n",
      "           7       0.35      0.58      0.44       631\n",
      "           8       0.72      0.90      0.80      8948\n",
      "           9       0.05      0.06      0.06     96996\n",
      "          10       0.01      0.01      0.01       569\n",
      "          11       0.28      0.41      0.34    481218\n",
      "          12       0.02      0.01      0.01      1075\n",
      "          13       0.23      0.35      0.28      5907\n",
      "          14       0.16      0.20      0.18      6239\n",
      "          15       0.05      0.02      0.03       623\n",
      "          16       0.17      0.19      0.18     17069\n",
      "          17       0.24      0.28      0.26     24416\n",
      "          18       0.01      0.01      0.01      5015\n",
      "          19       0.03      0.03      0.03      1641\n",
      "          20       0.11      0.10      0.10     15825\n",
      "          21       0.31      0.45      0.37     12148\n",
      "          22       0.25      0.28      0.26     24287\n",
      "          23       0.03      0.02      0.03      9684\n",
      "          24       0.01      0.01      0.01      2529\n",
      "          25       0.00      0.00      0.00       595\n",
      "          26       0.14      0.20      0.16      3692\n",
      "          27       0.11      0.10      0.10      1010\n",
      "          28       0.10      0.09      0.10     27023\n",
      "          29       0.01      0.01      0.01      8370\n",
      "          30       0.06      0.07      0.07     11868\n",
      "          31       0.06      0.04      0.05     14937\n",
      "          32       0.02      0.02      0.02      5055\n",
      "          33       0.14      0.15      0.14      8338\n",
      "          34       0.38      0.41      0.39     23655\n",
      "          35       0.05      0.07      0.06      1929\n",
      "          36       0.24      0.24      0.24     16373\n",
      "          37       0.23      0.21      0.22      6756\n",
      "          38       0.01      0.01      0.01      1761\n",
      "          39       0.01      0.00      0.00       492\n",
      "          40       0.54      0.60      0.57     52752\n",
      "          41       0.02      0.01      0.01     12631\n",
      "          42       0.36      0.41      0.38     17535\n",
      "          43       0.03      0.02      0.02      3896\n",
      "          44       0.05      0.03      0.03       458\n",
      "          45       0.05      0.02      0.03       622\n",
      "          46       0.26      0.25      0.26     45329\n",
      "          47       0.18      0.14      0.16      8422\n",
      "          48       0.47      0.43      0.45     49901\n",
      "          49       0.03      0.01      0.01       629\n",
      "          50       0.38      0.42      0.40     12618\n",
      "          51       0.35      0.30      0.33     16701\n",
      "          52       0.15      0.10      0.12       813\n",
      "          53       0.05      0.04      0.04       779\n",
      "          54       0.33      0.34      0.33      4083\n",
      "          55       0.03      0.01      0.02      4427\n",
      "          56       0.16      0.11      0.13      1459\n",
      "          57       0.34      0.29      0.31      7894\n",
      "          58       0.12      0.07      0.09    154095\n",
      "          59       0.28      0.14      0.19      1716\n",
      "          60       0.12      0.05      0.07      5618\n",
      "          61       0.27      0.15      0.19      3496\n",
      "          62       0.22      0.17      0.19     33963\n",
      "          63       0.17      0.12      0.14    119365\n",
      "          64       0.48      0.50      0.49     11204\n",
      "          65       0.00      0.00      0.00       652\n",
      "          66       0.38      0.41      0.40    696481\n",
      "          67       0.06      0.02      0.03     73989\n",
      "          68       0.20      0.08      0.12     84099\n",
      "          69       0.05      0.02      0.03     56020\n",
      "          70       0.26      0.16      0.20     64907\n",
      "          71       0.12      0.03      0.05      8244\n",
      "          72       0.05      0.01      0.01      2645\n",
      "          73       0.53      0.58      0.56     89922\n",
      "          74       0.11      0.04      0.06     19311\n",
      "          75       0.32      0.34      0.33    478209\n",
      "          76       0.30      0.18      0.22     75120\n",
      "          77       0.44      0.27      0.33      3574\n",
      "          78       0.46      0.50      0.48   1069619\n",
      "          79       0.17      0.05      0.08     12215\n",
      "          80       0.11      0.01      0.01      4825\n",
      "          81       0.02      0.00      0.00     16431\n",
      "          82       0.11      0.02      0.04     76405\n",
      "          83       0.01      0.00      0.00      3460\n",
      "          84       0.10      0.02      0.03      5335\n",
      "\n",
      "   micro avg       0.34      0.34      0.34   4273756\n",
      "   macro avg       0.18      0.17      0.17   4273756\n",
      "weighted avg       0.32      0.34      0.33   4273756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(df_for_model,labels,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianNB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 4.847206239878137\n",
      "Accuracy: 0.15960808244551164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      8626\n",
      "           1       0.00      0.00      0.00       541\n",
      "           2       0.00      0.00      0.00       515\n",
      "           3       0.00      0.00      0.00       498\n",
      "           4       0.01      0.08      0.02      1225\n",
      "           5       0.00      0.00      0.00       472\n",
      "           6       0.02      0.38      0.04      3336\n",
      "           7       0.01      0.13      0.01       631\n",
      "           8       0.06      0.38      0.11      8948\n",
      "           9       0.00      0.00      0.00     96996\n",
      "          10       0.00      0.00      0.00       569\n",
      "          11       0.20      0.16      0.18    481218\n",
      "          12       0.00      0.00      0.00      1075\n",
      "          13       0.00      0.00      0.00      5907\n",
      "          14       0.09      0.02      0.03      6239\n",
      "          15       0.11      1.00      0.20       623\n",
      "          16       0.00      0.00      0.00     17069\n",
      "          17       0.06      0.00      0.01     24416\n",
      "          18       0.00      0.00      0.00      5015\n",
      "          19       0.00      0.00      0.00      1641\n",
      "          20       0.02      0.00      0.00     15825\n",
      "          21       0.02      0.37      0.04     12148\n",
      "          22       0.01      0.05      0.01     24287\n",
      "          23       0.00      0.00      0.00      9684\n",
      "          24       0.00      0.00      0.00      2529\n",
      "          25       0.00      0.00      0.00       595\n",
      "          26       0.00      0.00      0.00      3692\n",
      "          27       0.00      0.00      0.00      1010\n",
      "          28       0.00      0.00      0.00     27023\n",
      "          29       0.02      0.01      0.01      8370\n",
      "          30       0.01      0.03      0.02     11868\n",
      "          31       0.00      0.00      0.00     14937\n",
      "          32       0.00      0.00      0.00      5055\n",
      "          33       0.00      0.00      0.00      8338\n",
      "          34       0.00      0.00      0.00     23655\n",
      "          35       0.00      0.36      0.00      1929\n",
      "          36       0.03      0.00      0.00     16373\n",
      "          37       0.00      0.00      0.00      6756\n",
      "          38       0.00      0.00      0.00      1761\n",
      "          39       0.00      0.00      0.00       492\n",
      "          40       0.14      0.09      0.11     52752\n",
      "          41       0.00      0.00      0.00     12631\n",
      "          42       0.00      0.00      0.00     17535\n",
      "          43       0.00      0.00      0.00      3896\n",
      "          44       0.00      0.00      0.00       458\n",
      "          45       0.00      0.00      0.00       622\n",
      "          46       0.00      0.00      0.00     45329\n",
      "          47       0.00      0.00      0.00      8422\n",
      "          48       0.00      0.00      0.00     49901\n",
      "          49       0.00      0.00      0.00       629\n",
      "          50       0.00      0.00      0.00     12618\n",
      "          51       0.00      0.00      0.00     16701\n",
      "          52       0.00      0.00      0.00       813\n",
      "          53       0.00      0.00      0.00       779\n",
      "          54       0.00      0.00      0.00      4083\n",
      "          55       0.00      0.00      0.00      4427\n",
      "          56       0.00      0.00      0.00      1459\n",
      "          57       0.00      0.00      0.00      7894\n",
      "          58       0.00      0.00      0.00    154095\n",
      "          59       0.00      0.00      0.00      1716\n",
      "          60       0.00      0.00      0.00      5618\n",
      "          61       0.00      0.00      0.00      3496\n",
      "          62       0.00      0.00      0.00     33963\n",
      "          63       0.00      0.00      0.00    119365\n",
      "          64       0.00      0.00      0.00     11204\n",
      "          65       0.00      0.00      0.00       652\n",
      "          66       0.35      0.14      0.21    696481\n",
      "          67       0.00      0.00      0.00     73989\n",
      "          68       0.00      0.00      0.00     84099\n",
      "          69       0.02      0.38      0.04     56020\n",
      "          70       0.00      0.00      0.00     64907\n",
      "          71       0.00      0.00      0.00      8244\n",
      "          72       0.00      0.00      0.00      2645\n",
      "          73       0.41      0.01      0.02     89922\n",
      "          74       0.00      0.00      0.00     19311\n",
      "          75       0.25      0.06      0.10    478209\n",
      "          76       0.03      0.00      0.00     75120\n",
      "          77       0.00      0.00      0.00      3574\n",
      "          78       0.29      0.41      0.34   1069619\n",
      "          79       0.00      0.00      0.00     12215\n",
      "          80       0.00      0.00      0.00      4825\n",
      "          81       0.00      0.00      0.00     16431\n",
      "          82       0.00      0.00      0.00     76405\n",
      "          83       0.00      0.00      0.00      3460\n",
      "          84       0.00      0.00      0.00      5335\n",
      "\n",
      "   micro avg       0.16      0.16      0.16   4273756\n",
      "   macro avg       0.03      0.05      0.02   4273756\n",
      "weighted avg       0.19      0.16      0.15   4273756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(df_for_model,labels,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
